# -*- coding: utf-8 -*-
"""Air quality & Chronic obstructive pulmonary disease (COPD).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZaevMi_oRBQYiLeVItUPDSUy7GuKSEC_
"""

!pip install --upgrade xlrd
!pip install pydot

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
import seaborn as sns
import itertools
from ipywidgets import interact, interactive
#import geopandas as gpd
import hashlib
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from scipy.stats import pearsonr
import re

sns.set(style="dark")
plt.style.use("ggplot")
# %matplotlib inline

"""Mounting drive."""

# from google.colab import files
# data_to_load = files.upload()

#make sure to run this cell first
from google.colab import drive
drive.mount('/content/drive')

"""# EDA
• Visualize at least two quantitative variables and two categorical variables. Your visualizations
must be relevant to your research questions!

• Describe any trends you observe, and any relationships you may want to follow up on.

• Describe any data cleaning steps you took. How will these decisions impact your models and
inferences?

• Explain how your visualizations should be relevant to your research questions: either by moti-
vating the question, or suggesting a potential answer. You must explain why they are relevant.
"""

copd = pd.read_csv("/content/drive/MyDrive/Data C102 final project/data/U.S._Chronic_Disease_Indicators__Chronic_Obstructive_Pulmonary_Disease.csv")
copd.head()
print(len(copd))
#if this cell doesn't run, "add shortcut to drive" will let you access this file (proj folder > right click > add shortcut)

"""COPD Data Cleaning Steps"""

copd.dropna()

#dropped meaningless columns
na_columns = ["Response","StratificationCategory2","Stratification2","StratificationCategory3","Stratification3","ResponseID","StratificationCategoryID2","StratificationID2","StratificationCategoryID3","StratificationID3"]
copd = copd.drop(na_columns, axis=1)

"""I have dropped columns with NA values which does not impact my model and inferences because they are meaningless columns. """

copd.describe()

copd['Question'].unique()

"""Since we are interested in the 'Prevalence of chronic obstructive pulmonary disease among adults >= 18'
filtered the dataframe that only contains this question.
"""

copd_question = copd[copd['Question']=='Prevalence of chronic obstructive pulmonary disease among adults >= 18']
copd_question.head()

copd_question.columns

select_columns = ['YearEnd',"LocationAbbr",'DataValue','Stratification1']
selected_copd_question = copd_question[select_columns]
selected_copd_question.head()

"""By observing the columns, our dataframe ended up with ['YearEnd',"LocationAbbr",'DataValue','Stratification1'] these columns since our model is interested whether the air quality affects the prevalance of chronic obstructive pulmonary disease.

## Data Visualization 1: Does smoking affects prevalance of COPD?
"""

sns.lineplot(data = selected_copd_question, x= "YearEnd", y = "DataValue").set(title = "Prevalance of Adults with COPD by Year");

smoking_copd = copd[copd['Question']=='Prevalence of current smoking among adults >= 18 with diagnosed chronic obstructive pulmonary disease']
sns.lineplot(data = smoking_copd, x= "YearEnd", y = "DataValue").set(title = "Prevalance of Smoking Adults with COPD by Year");

"""#### Explanation: 
It seems like both of the line plots are showing that the prevalance is gradyally decreasing over several years. Suprising fact is that while the lowest average prevalance value is around 6.4% in 2020, the value for the smoking adults was 37% around 2020. This implies that smoking is a significant factor that causes COPD among adults. 

I could use this implication to use smoking as a confounding factor for research question 1 since smoking both affects the prevalance of COPD among adults and the air quality. Also, it can act as our predictor for research question 2 since the smoking variable significantly affects the prevalance of COPD proven by the visualization.

## Data visualization 2: visualize how prevalance differs by race and gender (do they work as a predictor column for research question 2)
"""

g_copd_question = copd_question[copd_question['StratificationCategory1']=="Gender"]
r_copd_question = copd_question[copd_question['StratificationCategory1']== "Race/Ethnicity"]
g_copd_question.head()

sns.histplot(data = g_copd_question, x= "DataValue", hue = "Stratification1").set(title = "Prevalance of COPD by Gender");

sns.histplot(data = r_copd_question, x= "DataValue", hue = "Stratification1").set(title = "Prevalance of COPD by Race");

smoking_copd_g = smoking_copd[smoking_copd['StratificationCategory1']=="Gender"]
sns.histplot(data = smoking_copd_g, x= "DataValue", hue = "Stratification1").set(title = "Prevalance of COPD by Gender in smoking population");

"""Smoking is does not seem to be the critical factor that makes the difference in distribution of Prevalence of CPD by race. On distribution of Prevalence of COPD by gender, Female tends to have high prevelance of COPD."""

smoking_copd_r = smoking_copd[smoking_copd['StratificationCategory1']=="Race/Ethnicity"]
sns.histplot(data = smoking_copd_r, x= "DataValue", hue = "Stratification1").set(title = "Prevalance of COPD by Race in smoking population");

"""The sample sizes of each race has a huge difference. We should address the difference in the sample size when analyze the data

#### Explanation :
It seems like distribution of male's prevalance is located on the left of distribution of female's prevalance which implies that the average of males' prevalance is lower than females'. We could assume that female might have more probability to get diagonsed to COPD.

When I look at the Prevalance by Race, it seems like distribution of hispanic has relatively low prevalance than black and white. Also it seems like the distribution of white and black follows the similar distribution. Multiracial, non-hispanic seems to have relatively high prevalance than other races. 

Since it seems like the distribution of prevalance rate differeces by both race and gender, I could assume that these can act as a predictor to predict for research question 2.

# Ozone_Concentration Data set
"""

ozone = pd.read_csv("/content/drive/MyDrive/Data C102 final project/data/Daily_Census_Tract-Level_Ozone_Concentrations__2011-2014.csv")

ozone.head()

#ozone['year'].unique()
print(ozone['year'].value_counts())
ozone.tail(10)

"""Last 5 rows are missing/error data. Drop them"""

ozone = ozone.dropna(0)
ozone.tail()

"""## data cleaning: map statefips to statesabb

I have mapped states_codoes with state abbreviations to add column to both ozone and pm2.5 dataset in order to merge with COPD dataset.
"""

state_codes = {
    'WA': '53', 'DE': '10', 'DC': '11', 'WI': '55', 'WV': '54', 'HI': '15',
    'FL': '12', 'WY': '56', 'PR': '72', 'NJ': '34', 'NM': '35', 'TX': '48',
    'LA': '22', 'NC': '37', 'ND': '38', 'NE': '31', 'TN': '47', 'NY': '36',
    'PA': '42', 'AK': '02', 'NV': '32', 'NH': '33', 'VA': '51', 'CO': '08',
    'CA': '06', 'AL': '01', 'AR': '05', 'VT': '50', 'IL': '17', 'GA': '13',
    'IN': '18', 'IA': '19', 'MA': '25', 'AZ': '04', 'ID': '16', 'CT': '09',
    'ME': '23', 'MD': '24', 'OK': '40', 'OH': '39', 'UT': '49', 'MO': '29',
    'MN': '27', 'MI': '26', 'RI': '44', 'KS': '20', 'MT': '30', 'MS': '28',
    'SC': '45', 'KY': '21', 'OR': '41', 'SD': '46'
}
state_codes = dict(zip(state_codes.values(), state_codes.keys()))
state_codes = {int(k):v for k,v in state_codes.items()}
#state_codes

ozone['statefips'] = ozone['statefips'].fillna(0)
ozone['statefips'] = ozone['statefips'].astype('int')
ozone['stateAbbr'] = ozone['statefips'].map(state_codes)

ozone['countyfips'] = ozone['countyfips'].fillna(0)
ozone['countyfips'] = ozone['countyfips'].astype('int')
ozone.head()

"""1. 5 states with the highest ozone concentration
2. 5 states with the lowest ozone concentration

3. plot a histogram for all ozone concentration
"""

ozone_top_5 = ozone.groupby("stateAbbr")["ds_o3_pred"].mean().sort_values(ascending = False).head(5)
ozone_top_5

ozone_bottom_5 = ozone.groupby("stateAbbr")["ds_o3_pred"].mean().sort_values().head(5)
ozone_bottom_5

ozone['ds_o3_pred'].plot.hist()
plt.title("Distribution of Ozone Concentration")

"""It seems like the Ozone concentration distribution follows a unimodal normal distribution centered around 30-40.

Ozone concentration by countyfips
"""

grouped_ozone = ozone.groupby('countyfips').mean()
grouped_ozone.head()

plt.scatter(x=grouped_ozone['longitude'], y=grouped_ozone['latitude'], c= grouped_ozone['ds_o3_pred'])
plt.title('Ozone Concentration by County')
plt.colorbar()
plt.show()

"""#### Explanation: 
The higher the ozone concentration the more pollution, which seems like Colorado (CO), New Mexico (NM), Utah (UT), Arizona (AZ), Nevada (NV) area has high ozone concentration. 

I have implemented this visualization to check what geographical area has low air quality since it relates to both research question 1 and 2. 

Geographical region can act as an instrumental variable since it affects the air quality but not directly affects the prevalance of COPD. I chould also check if the prevalance of COPD is also high in these areas using the COPD dataset.
Also it could act as a predictor for research question 2 by creating dummy variable of each state since we could find air quality differs by states.

# PM 2.5_Concentration Data set
"""

#my notebook crashed while running first line; had to only take 5M rows
pmcon = pd.read_csv("/content/drive/MyDrive/Data C102 final project/data/Daily_Census_Tract-Level_PM2.5_Concentrations__2011-2014.csv", nrows = 5000000)

pmcon.head()

state_codes = {
    'WA': '53', 'DE': '10', 'DC': '11', 'WI': '55', 'WV': '54', 'HI': '15',
    'FL': '12', 'WY': '56', 'PR': '72', 'NJ': '34', 'NM': '35', 'TX': '48',
    'LA': '22', 'NC': '37', 'ND': '38', 'NE': '31', 'TN': '47', 'NY': '36',
    'PA': '42', 'AK': '02', 'NV': '32', 'NH': '33', 'VA': '51', 'CO': '08',
    'CA': '06', 'AL': '01', 'AR': '05', 'VT': '50', 'IL': '17', 'GA': '13',
    'IN': '18', 'IA': '19', 'MA': '25', 'AZ': '04', 'ID': '16', 'CT': '09',
    'ME': '23', 'MD': '24', 'OK': '40', 'OH': '39', 'UT': '49', 'MO': '29',
    'MN': '27', 'MI': '26', 'RI': '44', 'KS': '20', 'MT': '30', 'MS': '28',
    'SC': '45', 'KY': '21', 'OR': '41', 'SD': '46'
}
state_codes = dict(zip(state_codes.values(), state_codes.keys()))
state_codes = {int(k):v for k,v in state_codes.items()}
state_codes

pmcon['statefips'] = pmcon['statefips'].fillna(0)
pmcon['statefips'] = pmcon['statefips'].astype('int')
pmcon['stateAbbr'] = pmcon['statefips'].map(state_codes)
pmcon.head()

"""1. 5 states with the highest PM 2.5 concentration
2. 5 states with the lowest PM 2.5 concentration

3. plot a histogram for all PM 2.5 concentration
"""

pmcon_top_5 = pmcon.groupby("stateAbbr")["ds_pm_pred"].mean().sort_values(ascending = False).head(5)
pmcon_top_5

pmcon_bottom_5 = pmcon.groupby("stateAbbr")["ds_pm_pred"].mean().sort_values().head(5)
pmcon_bottom_5

sns.histplot(data = pmcon, x ="ds_pm_pred", binwidth = 3);
#pmcon['ds_pm_pred'].plot.hist()

pmcon[pmcon['ds_pm_pred']>=40]['year'].value_counts()

grouped_pmcon = pmcon.groupby('countyfips').mean()
plt.scatter(x=grouped_pmcon['longitude'], y=grouped_pmcon['latitude'], c= grouped_pmcon['ds_pm_pred'])
plt.title("PM 2.5 Concentration by County")
plt.colorbar()
plt.show()

"""It seems like year 2014 contains the most number of pm2.5 concentration that is higher than 40 while the data visualization doesn't consider these factors which is just averaging the value of PM 2.5 concentration for the entire year.

According to this visualization it shows that Illinois, Indiana area has high PM 2.5 concentration.

## Data Visualization 3
"""

copd_question_ageadj = copd_question[copd_question['DataValueTypeID'] == 'AGEADJPREV']

#COPD Age Adjusted Prevalence Geomap 
state_prev = copd_question_ageadj[['LocationAbbr','DataValue']].groupby('LocationAbbr').mean().reset_index()

fig = go.Figure(data=go.Choropleth(
    locations=state_prev['LocationAbbr'], # Spatial coordinates
    z = state_prev['DataValue'].astype(float), # Data to be color-coded
    locationmode = 'USA-states', # set of locations match entries in `locations`
    colorscale = 'Viridis',
    colorbar_title = "Age Adjusted Prevalence %",
))
fig.update_layout(
    title_text = 'Age Adjusted Prevalence by State',
    geo_scope='usa', # limit map scope to USA
    margin={"r":0,"t":30,"l":0,"b":0}
)

fig.show()

#Pm2.5 geomap
pm_conc_states = pmcon[['stateAbbr','ds_pm_pred']].groupby('stateAbbr').mean().reset_index()

fig = go.Figure(data=go.Choropleth(
    locations= pm_conc_states['stateAbbr'], # Spatial coordinates
    z = pm_conc_states['ds_pm_pred'].astype(float), # Data to be color-coded
    locationmode = 'USA-states', # set of locations match entries in `locations`
    colorscale = 'Viridis',
    colorbar_title = "average PM2.5 concentration in μg/m3",
    
))
fig.update_layout(
    title_text = 'Mean estimated 24-hour average PM2.5 concentration in μg/m3 by States',
    geo_scope='usa', # limit map scope to USA
    margin={"r":0,"t":30,"l":0,"b":0}
)

fig.show()

#Ozone geomap
ozone_states = ozone[['stateAbbr','ds_o3_pred']].groupby('stateAbbr').mean().reset_index()

fig = go.Figure(data=go.Choropleth(
    locations= ozone_states['stateAbbr'], # Spatial coordinates
    z = ozone_states['ds_o3_pred'].astype(float), # Data to be color-coded
    locationmode = 'USA-states', # set of locations match entries in `locations`
    colorscale = 'Viridis',
    colorbar_title = "ozone ppb",
    
))
fig.update_layout(
    title_text = 'Mean estimated 8-hour ozone ppb by States',
    geo_scope='usa', # limit map scope to USA
    margin={"r":0,"t":30,"l":0,"b":0}
)

fig.show()

"""**Trend**

These color coded geomaps show how age adjusted prevalence rate of COPD, ozone concentration, and pm 2.5 concentration differs by states. By looking at geomaps of COPD prevalence rate and pm 2.5 concentration, we could observe that regions with high pm 2.5 concentration (Kentucky, West Virginia, etc) also tend to have high COPD prevalence rate. This indicates that there exists some relationship between pm 2.5 and COPD rate; we aim to define this relationship using causal inference technique. It was interesting to see that regions with higher ozone concentration (Nevada, Arizona, etc.) did not necessarily have higher COPD rate. 

**Data cleaning**

Since the granularity of COPD data is in each states, we grouped ozone & pm 2.5 table by states as well, by taking the mean. During the process, ozone & pm 2.5 data could have lost some information. For instance, while taking the mean, we could have accidentally included outliers. 

**Relevance**

This is relevant to our research question regarding the relationship between air pollutant and age adjusted prevalence rate of COPD. The visualization clearly shows that states with higher pm 2.5 have higher COPD rate.

## Data Visualization 4
"""

pmcon_year = pmcon[['stateAbbr','ds_pm_pred']].groupby('stateAbbr').mean().reset_index()
ozone_year = ozone[['stateAbbr','ds_o3_pred']].groupby('stateAbbr').mean().reset_index()
bad_air_state = ozone_year.merge(pmcon_year, how = 'inner', on = ['stateAbbr']).sort_values(by = ['ds_pm_pred', 'ds_o3_pred'], ascending = False).head(5)
good_air_state = ozone_year.merge(pmcon_year, how = 'inner', on = ['stateAbbr']).sort_values(by = ['ds_pm_pred', 'ds_o3_pred'], ascending = True).head(5)

bad_air_state

plt.figure(figsize=(8, 6))
copdStates = copd_question_ageadj[['LocationAbbr','DataValue','StratificationCategoryID1']].groupby(['LocationAbbr','StratificationCategoryID1']).mean().reset_index()
bad_air_copd = copdStates[copdStates['LocationAbbr'].isin(bad_air_state['stateAbbr'].to_list())]
good_air_copd = copdStates[copdStates['LocationAbbr'].isin(good_air_state['stateAbbr'].to_list())]
sns.barplot(x="States", y="Age-adj Prevalence Rate(%)", hue="Category", data= bad_air_copd.rename(columns = {'LocationAbbr': 'States', 'DataValue':'Age-adj Prevalence Rate(%)','StratificationCategoryID1':'Category'}).sort_values(by = 'Age-adj Prevalence Rate(%)'));
plt.title('States with Bad Air Quality: Average Age Adjusted Prevalence Rate');

plt.figure(figsize=(8, 6))
sns.barplot(x="States", y="Age-adj Prevalence Rate(%)", hue="Category", data= good_air_copd.rename(columns = {'LocationAbbr': 'States', 'DataValue':'Age-adj Prevalence Rate(%)','StratificationCategoryID1':'Category'}).sort_values(by = 'Age-adj Prevalence Rate(%)'));
plt.title('States with Good Air Quality: Average Age Adjusted Prevalence Rate');

"""**Trend**

These two bar plots each compare age adjusted prevalence rate of COPD in states with good and bad air quality. We could observe that states with bad air quality have higher age adjusted prevalence rate of COPD compared to that of states with good air quality. For instance, COPD rate in states with bad air quality ranges from 6% to 14%, while that of in good air quality states ranges from 4% to 7.8%. 

**Data cleaning**

As stated in the research question, we defined bad air quality as ozone concentration greater than 80ppb and PM 2.5 concentration greater than 35.4μg/m3. However, since ozone & pm2.5 table for this visualization only contained 5M rows each, the data was not comprehensive enough to allow strict classification of good and bad air quality. Thus, we took the yearly average ozone/pm2.5 concentration by each state and defined five bad/good air quality states as those with highest/lowest ozone and pm2.5 concentration, respectively. 

**Relevance**

This is relevant to our research question regarding the relationship between air pollutant and age adjusted prevalence rate of COPD. The visualization clearly shows that states with bad air quality have higher COPD rate.

https://www.census.gov/newsroom/releases/archives/2010_census/cb11-cn185.html

https://www.scientificamerican.com/article/past-racist-redlining-practices-increased-climate-burden-on-minority-neighborhoods/

https://en.wikipedia.org/wiki/List_of_U.S._states_by_non-Hispanic_white_population

https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_African-American_population

Will further examine 'climate burden on minority neighborhoods'

## Data Visualization 5
"""

# minority = ['NY','FL','TX','GA','CA','NC','IL','MD','VA','OH']
minority = ['VI','DC','MS','LA','GA','MD']
# white = ['ME','VT','WV','NH','ND','IA','MT','KY','WY','SD']
white = ['ME','VT','WV','NH','ND']

minority_ozone = ozone[ozone['stateAbbr'].isin(minority)][['stateAbbr','ds_o3_pred']].groupby('stateAbbr').mean().reset_index()
white_ozone = ozone[ozone['stateAbbr'].isin(white)][['year','ds_o3_pred']].groupby('year').mean().reset_index().rename(columns = {'year':'stateAbbr'})
white_ozone['stateAbbr'] = white_ozone['stateAbbr'].replace({2011:'white'})
white_ozone = white_ozone[['stateAbbr','ds_o3_pred']].groupby('stateAbbr').mean().reset_index()
ozone_neigh = pd.concat([minority_ozone, white_ozone])

#bar plot comparing white and black neighborhoods
plt.figure(figsize=(8, 6))
sns.barplot(x="States", y="Ozone(ppb)", data=ozone_neigh.rename(columns = {'stateAbbr':'States','ds_o3_pred':'Ozone(ppb)'}).sort_values(by = 'Ozone(ppb)'));
plt.title('Comparison of Ozone ppb in Non-Hispanic White vs Black Neighborhood');

minority_pmcon = pmcon[pmcon['stateAbbr'].isin(minority)][['stateAbbr','ds_pm_pred']].groupby('stateAbbr').mean().reset_index()
white_pmcon = pmcon[pmcon['stateAbbr'].isin(white)][['year','ds_pm_pred']].groupby('year').mean().reset_index().rename(columns = {'year':'stateAbbr'})
white_pmcon['stateAbbr'] = white_pmcon['stateAbbr'].replace({2011:'white'})
white_pmcon = white_pmcon[['stateAbbr','ds_pm_pred']].groupby('stateAbbr').mean().reset_index()
ozone_neigh = pd.concat([minority_pmcon, white_pmcon])

#bar plot comparing white and black neighborhoods
plt.figure(figsize=(8, 6))
sns.barplot(x="States", y="pm2.5", data=ozone_neigh.rename(columns = {'stateAbbr':'States','ds_pm_pred':'pm2.5'}).sort_values(by = 'pm2.5'));
plt.title('Comparison of pm 2.5: Non-Hispanic White vs Black Neighborhood');

"""**Trend**

These two bar plots each compare ozone ppb and pm2.5 concentration in non-hispanic white and Black neighborhoods (top five states with highest non-hispanic/black population, according to census data). Here, we could observe that both ozone ppb and pm2.5 concentration in white neighborhood are relatively lower than that of in Black neighborhood. This trend may suggest that the climate burden exists in minority neighborhood. This may also imply that since people of color are easily exposed to air pollutants, they are more susceptible to health consequences.

**Data cleaning**

Due to memory shortage, I only took 5 million rows for each ozone and pm 2.5 data. This may have affected data coverage thus harm credibility of the data, since it only contained a portion of 2011 data. 

**Relevance**

This is relevant to our research question regarding the relationship between geographical location, race, and COPD prevalence. Starting from here, we can further examine if most black populated states have a higher chance of getting diagnosed with COPD. From the visualization, we have identified how racial composition differs by states and how these states have different levels of air pollutant. 

"""

age_data =pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/nst-est2019-01.csv', skiprows=3)[5:55]

age_data.columns

age_data = age_data.rename(columns={'Unnamed: 0':'State'})
age_data['State'] = age_data['State'].str.replace('.','')
age_data.head()

state_ab = pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/state_ab.csv')
state_ab.head()

age_data = age_data.merge(state_ab, how='inner', left_on='State', right_on='State').drop(columns=['Abbrev'])
age_data['Census'] = age_data['Census'].str.replace(',', '').astype(int)

fig_age = go.Figure(data=go.Choropleth(
    locations= age_data['Code'], 
    z = age_data['Census'], 
    locationmode = 'USA-states', 
    colorscale = 'Viridis',
    colorbar_title = "census age",
    
))
fig.update_layout(
    title_text = 'Census age by States',
    geo_scope='usa',
    margin={"r":0,"t":30,"l":0,"b":0}
)

fig.show()

age_data.sort_values('Census', ascending=False).head()

"""# Research Question 1: Does bad air quality increase the age-adjusted prevalence of COPD?

## Greenhouse gas emission dataset (confounding)
"""

greenhouse = pd.read_excel("/content/drive/MyDrive/Data C102 final project/data/flight.xls", sheet_name='2011')
index = greenhouse.iloc[5]
index_dict = dict()
for i in range(len(index)):
  index_dict[greenhouse.columns[i]] = index[i]
index_dict
greenhouse = greenhouse.rename(columns=index_dict)
greenhouse = greenhouse.dropna()
greenhouse = greenhouse[1:]
greenhouse.head()

def plot_long_lat(df, col_name, title):
  plt.scatter(x=df['LONGITUDE'], y=df['LATITUDE'], c= df[col_name])
  plt.title(title)
  plt.colorbar()
  plt.show();

plot_long_lat(greenhouse, "GHG QUANTITY (METRIC TONS CO2e)", "GH emission")

is_out_of_lat = (greenhouse["LATITUDE"] > 50) | (greenhouse["LATITUDE"] < 20)
is_out_of_long = (greenhouse["LONGITUDE"] < -150) | (greenhouse["LONGITUDE"] > -50)
is_out_of_latlong = is_out_of_lat| is_out_of_long
greenhouse[is_out_of_latlong]['STATE'].value_counts()

is_known_state = (greenhouse['STATE'] == 'PR') | (greenhouse['STATE'] == 'AK') | (greenhouse['STATE'] == 'HI') | (greenhouse['STATE'] == 'VI') | (greenhouse['STATE'] == 'GU')
is_mainland = ~(is_out_of_latlong | is_known_state)
greenhouse_mainland = greenhouse[is_mainland]
plot_long_lat(greenhouse_mainland, "GHG QUANTITY (METRIC TONS CO2e)", "GH emission")

states = ('PR', 'AK', 'HI', 'VI', 'GU')
pmcon_p = pmcon[['year',"stateAbbr","ds_pm_pred"]].groupby(['year',"stateAbbr"]).mean().reset_index()
ozone_p = ozone[['year',"stateAbbr","ds_o3_pred"]].groupby(['year',"stateAbbr"]).mean().reset_index()

merged_causal = pd.merge(pmcon_p,ozone_p, on = ['year',"stateAbbr"], how = 'inner')

for i in merged_causal['stateAbbr']:
  if i in states:
    print(i)

intersection = set(greenhouse_mainland['STATE']).intersection(set(states))
intersection

greenhouse_mainland = greenhouse_mainland.groupby("STATE").mean().drop(columns=['ZIP CODE', 'GHGRP ID']).rename(columns={'GHG QUANTITY (METRIC TONS CO2e)': 'GHG QUANTITY'})
plot_long_lat(greenhouse_mainland, "GHG QUANTITY", "GH emission per state")
greenhouse_mainland = greenhouse_mainland["GHG QUANTITY"].reset_index()
greenhouse_mainland.head()

states = {
        'AK': 'Alaska',
        'AL': 'Alabama',
        'AR': 'Arkansas',
        'AS': 'American Samoa',
        'AZ': 'Arizona',
        'CA': 'California',
        'CO': 'Colorado',
        'CT': 'Connecticut',
        'DC': 'District of Columbia',
        'DE': 'Delaware',
        'FL': 'Florida',
        'GA': 'Georgia',
        'GU': 'Guam',
        'HI': 'Hawaii',
        'IA': 'Iowa',
        'ID': 'Idaho',
        'IL': 'Illinois',
        'IN': 'Indiana',
        'KS': 'Kansas',
        'KY': 'Kentucky',
        'LA': 'Louisiana',
        'MA': 'Massachusetts',
        'MD': 'Maryland',
        'ME': 'Maine',
        'MI': 'Michigan',
        'MN': 'Minnesota',
        'MO': 'Missouri',
        'MP': 'Northern Mariana Islands',
        'MS': 'Mississippi',
        'MT': 'Montana',
        'NA': 'National',
        'NC': 'North Carolina',
        'ND': 'North Dakota',
        'NE': 'Nebraska',
        'NH': 'New Hampshire',
        'NJ': 'New Jersey',
        'NM': 'New Mexico',
        'NV': 'Nevada',
        'NY': 'New York',
        'OH': 'Ohio',
        'OK': 'Oklahoma',
        'OR': 'Oregon',
        'PA': 'Pennsylvania',
        'PR': 'Puerto Rico',
        'RI': 'Rhode Island',
        'SC': 'South Carolina',
        'SD': 'South Dakota',
        'TN': 'Tennessee',
        'TX': 'Texas',
        'UT': 'Utah',
        'VA': 'Virginia',
        'VI': 'Virgin Islands',
        'VT': 'Vermont',
        'WA': 'Washington',
        'WI': 'Wisconsin',
        'WV': 'West Virginia',
        'WY': 'Wyoming'
}


states_reverse = dict(zip(states.values(), states.keys()))
states_reverse = {k:v for k,v in states_reverse.items()}
states_reverse

"""## wildfire dataset (Confounding Variable)"""

wildfire = pd.read_csv("/content/drive/MyDrive/Data C102 final project/data/wildfire.csv")
wildfire.head()
#len(wildfire)

wildfire_eda = wildfire[['state',"disc_pre_year","fire_mag",'latitude',"longitude"]][wildfire['disc_pre_year'] == 2011]
wildfire_eda.head()
plt.scatter(x=wildfire_eda['longitude'], y=wildfire_eda['latitude'], c= wildfire_eda['fire_mag'], s= 7);
plt.title("Wildfire Magnitude by State")
plt.colorbar()
plt.show()

#https://www.kaggle.com/datasets/capcloudcoder/us-wildfire-data-plus-other-attributes?select=Wildfire_att_description.txt
# extract meaningful columns
fire_columns = ['state',"disc_pre_year","fire_mag"]
wildfire_n = wildfire[fire_columns][wildfire['disc_pre_year'] == 2011]
wildfire_df = wildfire_n.groupby('state',as_index = False).agg({'fire_mag': np.mean, "disc_pre_year": 'count'}).rename(columns = {'disc_pre_year':"wildfire_occurences"}).fillna(0)
wildfire_df.head()

#depending on the fire magnitude/ fire size and number of occurences the airquality will differ

"""## Smoking Dataset (confounding variable)"""

#https://ghdx.healthdata.org/record/ihme-data/united-states-smoking-prevalence-county-1996-2012
#dataset contains smoking prevalance in US counties
smoking_temp = pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/IHME_US_COUNTY_TOTAL_AND_DAILY_SMOKING_PREVALENCE_1996_2012.csv')
smoking_temp = smoking_temp[smoking_temp['year']==2011]
smoking_temp = smoking_temp.groupby('state').mean().reset_index()
smoking_temp = smoking_temp[['state',"total_mean"]]
smoking_temp = smoking_temp.rename(columns={"total_mean":"smoking_prevalance"})
#smoking_temp.describe()
copd_smoking = copd[copd['Question']=='Prevalence of chronic obstructive pulmonary disease among adults >= 18']
#copd_question.head()
copd_question_temp = copd_smoking.merge(smoking_temp, left_on = "LocationDesc",right_on = "state")
copd_question_temp.head()

smoking_temp = pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/IHME_US_COUNTY_TOTAL_AND_DAILY_SMOKING_PREVALENCE_1996_2012.csv')
smoking_temp = smoking_temp[smoking_temp['year']==2011]
smoking_temp

copd_causal = copd_question_temp[copd_question_temp["DataValueType"] == "Age-adjusted Prevalence"]
copd_causal = copd_causal[["YearEnd",'LocationAbbr','DataValue',"smoking_prevalance"]]
copd_causal = copd_causal[copd_causal['YearEnd']==2011]
copd_causal = copd_causal.groupby("LocationAbbr").mean().reset_index()
copd_causal.head()

#pmcon & ozone only contains year 2011
pmcon_p = pmcon[["stateAbbr","ds_pm_pred"]].groupby("stateAbbr").mean().reset_index()
ozone_p = ozone[ozone['year']==2011][["stateAbbr","ds_o3_pred"]].groupby("stateAbbr").mean().reset_index()


#merge dataset
merged_df = copd_causal.merge(pmcon_p,left_on="LocationAbbr",right_on = "stateAbbr")
merged_df = merged_df.merge(ozone_p,on = "stateAbbr")
merged_df = merged_df.merge(greenhouse_mainland, left_on='stateAbbr', right_on='STATE')
merged_df = merged_df.merge(wildfire_df, left_on = "stateAbbr",right_on = 'state', how = 'left')
merged_df.head()

merged_df.describe()

merged_df_temp = merged_df[['DataValue','smoking_prevalance',"ds_pm_pred","ds_o3_pred"]]
fig = sns.pairplot(merged_df_temp,  plot_kws=dict(marker="o", alpha = 0.5))
for i, j in zip(*np.triu_indices_from(fig.axes, 1)):
    fig.axes[i, j].set_visible(False)
plt.show()

"""## Population Dataset"""

pop_dens = pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/popdens.csv')
print(pop_dens.columns)

pop_dens = pd.read_csv('/content/drive/MyDrive/Data C102 final project/data/popdens.csv')
pop_dens = pop_dens.iloc[5:56,:][['NAME',"POPESTIMATE2011"]]
pop_dens['NAME'] = pop_dens['NAME'].map(states_reverse)
pop_dens.head()

merged_df = merged_df.merge(pop_dens, left_on = "LocationAbbr",right_on = "NAME")
#merged_df = merged_df.drop(columns = ['stateAbbr',"YearEnd","STATE",'state'])
merged_df.head()

fig = sns.pairplot(merged_df[['DataValue',"ds_pm_pred","ds_o3_pred","POPESTIMATE2011"]],  plot_kws=dict(marker="o", alpha = 0.5))
for i, j in zip(*np.triu_indices_from(fig.axes, 1)):
    fig.axes[i, j].set_visible(False)
plt.show()

"""### Data Set preparation for causal inference
1. copd 
  - only contains year 2011
  - filtered out the data which only contains the information of age-adjusted prevalence of COPD among adults greater than 18 years old (question column)

2. ozone & pmcon
  - only contains year 2011
  - filtered out data that only contains the year 2011 and grouped the dataset by state and computed the mean concentration of each state. 
3. greenhouse gas emission, smoking, wildfire, population density
  - contains the info by states in year 2011

4. merged_df
 - merged dataframe of copd, ozone, pmcon, and smoking dataframe

### Describe which variables correspond to treatment and outcome.
Our treatment is whether the state has bad air quality. Treatment = 1 is the top 16 states that have the highest pm 2.5 and ozone concentration. Our outcome is the data value of age-adjusted prevalence of COPD among adults greater than 18 years old. 

### Describe which variables (if any) are confounders. If the unconfoundedness assumption holds, make a convincing argument for why.
Our confounding variable are the smoking prevalence and green house emission of each state. As you can see on the above pairplot we have noticed an association between confounding variables and prevalence of COPD also between smoking prevalence and the air quality concentration. We have decided to use inverse propensity weighting to adjust for confounders.
"""

#going to sort pm and ozone concentration values and set states with bad/good air quality 
merged_df = merged_df.sort_values(by = ['ds_pm_pred', 'ds_o3_pred'], ascending = [False, False])
bad_air_quality_states = merged_df['LocationAbbr'].unique()[:17]
merged_df['treatment'] = [1 if a in bad_air_quality_states else 0 for a in merged_df['LocationAbbr']]

merged_df.head()

#Calculate Simple Difference in Observed group mean
sdo = np.mean(merged_df[merged_df['treatment']==1]['DataValue'])-np.mean(merged_df[merged_df['treatment']==0]['DataValue'])
sdo

#hower we can't use the simple difference in observed group means (SDO) to estimate the causal ATE

"""the SDO is positive, however, we can't use the simple difference in observed group means (SDO) to estimate the causal ATE.

## Unconfoundedness Techniques
"""

merged_df = merged_df.fillna(0)
merged_df.describe()

z = merged_df['treatment'].array
y = merged_df['DataValue'].array
x = merged_df[['smoking_prevalance',"GHG QUANTITY","fire_mag","wildfire_occurences","POPESTIMATE2011"]].to_numpy()

"""### Outcome Regression

Suppose we fit a linear model of the following form:

Prevalance = τ * Z + a* Prevalance of Smoking + b * Greenhouse Gas emission + c * Magnitude of Fire + d * Occurrences of wildfire + e * Population density of 2011

Under the following assumptions, the estimated coefficient of treatment from OLs, predicted τ, will be an unbiased estimate of the ATE:

1. Assume unconfoundedness given this set of 6 variables.
2. Assume this new linear model correctly describes the interaction between the variables.
"""

#code extracted from DATA102 LAB
def fit_OLS_model(df, target_variable, explanatory_variables, intercept = False):
    target = df[target_variable]
    inputs = df[explanatory_variables]
    if intercept:
        inputs = sm.add_constant(inputs)
    
    fitted_model = sm.OLS(target, inputs).fit()
    return(fitted_model)
def mean_squared_error(true_vals, predicted_vals):
    return np.mean((true_vals - predicted_vals) ** 2)

linear_model = fit_OLS_model(merged_df, 'DataValue',['treatment','smoking_prevalance',"GHG QUANTITY","fire_mag","wildfire_occurences","POPESTIMATE2011"])
print(linear_model.summary())

"""Since the confounding variables do not all have a linear effect on prevalance of COPD, we thought that our assumption 2 is questionable since our coefficient of treatment of OLS is 0.2028 which is even smaller than the simple difference in observed mean.

#### Inverse Propsensity Weighting
"""

z = merged_df['treatment'].array
y = merged_df['DataValue'].array
x = merged_df[['smoking_prevalance',"GHG QUANTITY","fire_mag","wildfire_occurences","POPESTIMATE2011"]].to_numpy()

from sklearn.linear_model import LogisticRegression as LR
lr = LR(penalty="none", max_iter=200, random_state=0)
lr.fit(x,z)

p_score = lr.predict_proba(x)[:,1]

merged_df['pscore'] = p_score
merged_df.head()

plt.hist(merged_df[merged_df['treatment']==0]['pscore'], label = 'good air quality');
plt.hist(merged_df[merged_df['treatment']==1]['pscore'], label = "bad air quality");
plt.title("Propensity score of states with bad air quality");
print("pscore_treatment =1:", np.mean(merged_df[merged_df['treatment']==1]['pscore']))
print("pscore_treatment =0:", np.mean(merged_df[merged_df['treatment']==0]['pscore']))
plt.legend();

"""It seems like the propensity score for states with good air quality has a

2. calculate IPW estimate for the ATE
"""

reweighted_treated = np.sum(merged_df[merged_df['treatment']==1]['DataValue']/merged_df[merged_df['treatment']==1]['pscore'])/len(merged_df)
reweighted_control = np.sum(merged_df[merged_df['treatment']==0]['DataValue']/merged_df[merged_df['treatment']==1]['pscore'])/len(merged_df)
ipw_estimate = reweighted_treated-reweighted_control
ipw_estimate

"""## Results
### Summarize and interpret your results, providing a clear statement about causality (or a lack thereof) including any assumptions necessary.

If we assume that the age-adjusted prevalence of COPD among adults is unconfounded given the variables and the propensity score is a good model of Probability of states has a bad quality given the smoking prevalence, then the estimated effect of bad air quality is that the bad air quality causes people to have 6.8% more than they would have.  

### Where possible, discuss the uncertainty in your estimate and/or the evidence against the hypotheses you are investigating.
Since the world is complicated there might be missing confounders not taken care of by the data and model.

# Research Question 2: Can we predict whether people have chronic diseases (COPD) from geographical (location), race/ethnicity trends?

### Data Preparation for GLM

Data Preparation for GLM
"""

copd = pd.read_csv("/content/drive/MyDrive/Data C102 final project/data/U.S._Chronic_Disease_Indicators__Chronic_Obstructive_Pulmonary_Disease.csv")
#dropped meaningless columns
na_columns = ["Response","StratificationCategory2","Stratification2","StratificationCategory3","Stratification3","ResponseID","StratificationCategoryID2","StratificationID2","StratificationCategoryID3","StratificationID3"]
copd = copd.drop(na_columns, axis=1)

copd_question = copd[copd['Question']=='Prevalence of chronic obstructive pulmonary disease among adults >= 18']
copd_question_ageadj = copd_question[copd_question['DataValueTypeID'] == 'AGEADJPREV']

longlat = pd.read_csv('https://raw.githubusercontent.com/google/dspl/master/samples/google/canonical/states.csv')
longlat.loc[52] = ['GU', 13.444304, 144.793732, 'Guam']
longlat

copd_rq2 = copd_question_ageadj[["LocationAbbr", 'DataValue', 'Stratification1']][copd_question_ageadj['StratificationCategory1'] == 'Race/Ethnicity']
copd_rq2 = copd_rq2.dropna()
one_hot = pd.get_dummies(copd_rq2['Stratification1'])
copd_rq2 = copd_rq2.join(one_hot)
copd_rq2 = copd_rq2.rename(columns={'Black, non-Hispanic':"Black", "Multiracial, non-Hispanic" : "Multiracial", "Other, non-Hispanic": "Other", "White, non-Hispanic":"White"})
copd_rq2['DataValue'] = copd_rq2['DataValue'].astype(float)
copd_rq2['likely'] = 1*(copd_rq2['DataValue'] > 6.4)
copd_rq2 = copd_rq2.reset_index().drop(columns=['Stratification1', 'DataValue','index'])
copd_rq2 = copd_rq2.merge(longlat, left_on = 'LocationAbbr', right_on = 'state').drop(columns=['state'])
copd_rq2

copd_rq2 = copd_rq2.rename(columns={'likely':'y'})

"""##GLM: Bayesian Logistic Regression Model"""

!pip install pymc3

#bayesian
import pymc3 as pm

with pm.Model() as logistic_model:
    pm.glm.GLM.from_formula(
        "y ~ latitude + longitude + Black + Hispanic + Multiracial + Other + White", data=copd_rq2, family=pm.glm.families.Binomial()
    )
    trace = pm.sample(1000, init='adapt_diag', target_accept = 0.95, cores=2, random_seed=33)

import arviz as az
az.plot_posterior(trace, ['Intercept', 'latitude','longitude','Black','Hispanic','Multiracial','Other','White'], round_to = 3)
plt.show()

with logistic_model:
  logistic_ppc = pm.sample_posterior_predictive(trace)
  logistic_ppc['y'] = logistic_ppc['y'] + 0.0
  ppc_logistic = az.from_pymc3(trace, posterior_predictive=logistic_ppc)
# Plot PPC samples
fig, ax = plt.subplots(figsize=(12, 6))
az.plot_ppc(ppc_logistic, ax=ax)
plt.xlabel('y = Likelihood of Having COPD')

plt.title('Bayesian Posterior Predictive Check')
plt.show()

az.summary(trace)

az.hdi(trace, hdi_prob=0.95)

"""### Results
**– Summarize and interpret the results from your models.**

Looking at the PPC diagram below, It seems like our posterior predictive is really close to our actual values of the data; however, it also seems like that the model has overfit to the data as all the values are within the posterior predictive. 

Looking at the posterior distributions of features, individuals who are multiracial are most likely to have COPD ceteris paribus. Moreover, it seems like the distributions of latitude and longitude are fairly normal at around 0, with small values of standard deviations. However, the other variables are very skewed and have very large values of standard deviations. 


**– Estimate any uncertainty in your GLM predictions, providing clear quantitative statements of the uncertainty in plain English.**

The numbers in the diagram below are 95% credible intervals for each of the features of the model; 95% of the posterior distribution is in the credible intervals for each of the features below. So, for example, in the diagram above, we can see the posterior distributions of the feature “intercept”. Now looking at the diagram below, we can see that the 95% credible interval for “intercept” is [-916.2, -76.01]. What this means is that about 95% of the posterior distribution for “intercept” is between the [-916.2, -76.01].

##GLM: Frequentist Logistic Regression Model
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf

freq_model = smf.glm("y ~ latitude + longitude + Black + Hispanic + Multiracial + Other + White", data = copd_rq2, family = sm.families.Binomial())

freq_res = freq_model.fit(  )
print(freq_res.summary())

"""### Results
**– Summarize and interpret the results from your models.**

From the frequentist model, we were able to see that hispanic and white individuals are less likely to have COPD compared to other races, ceteris paribus (hispanic and white's coefficients are negative). Moreover, as the state's latitude is lower and its longitude is higher, individuals in that state are more likely than individuals in other states. The intercept of the model is 0.7761, which means that the model will provide an output of 0.7761, if all other variables are 0; however, it is not possible for all the variables to be 0 as all of the data in our dataset is one of the five races. Moreover, a longitude of 0 and latitude of 0 would indicate a location in the Atlantic Ocean, which is obviously not included in our data.

Moreover, the p-values of latitude, longitude, Black, and Other are greater than 0.05, meaning that they are not statistically significant at the 5% level. The variables that are statistically significant at the 5% level are Hispanic, Multiracial, and White. 

With the average log-likelihood being about -0.57, which is relatively close to 0, our model may be a good fit for the data. However, the value of pearson chi-squared, 1590, is really high meaning that our model may not be a good fit for the data. 



**– Estimate any uncertainty in your GLM predictions, providing clear quantitative statements of the uncertainty in plain English.**

The numbers in the left two columns in the table are 95% confidence intervals for each of the features. The concept of confidence intervals is that the probability of the true value of the features’ coefficient falling into the interval is about 95%. For example, the confidence interval of the feature “intercept” is [0.185, 1.367]. Thus, the probability of the true coefficient of “intercept” being in the interval of [0.185, 1.367] is 95%.

##Non-Parametric Method: Random Forest

###Data Preparation: merge Longitude/Latitude Data & one-hot encoding
"""

#import longitude and latitude data
longlat = pd.read_csv('https://raw.githubusercontent.com/google/dspl/master/samples/google/canonical/states.csv')
#Add Guam; source: https://www.latlong.net/place/guam-18392.html
longlat.loc[52] = ['GU', 13.444304, 144.793732, 'Guam']

#data preparation: one-hot encoding, merge longitude/latitude data with COPD data
copd_rq2 = copd_question_ageadj[["LocationAbbr", 'DataValue', 'Stratification1', 'YearStart']][copd_question_ageadj['StratificationCategory1'] == 'Race/Ethnicity']
copd_rq2 = copd_rq2.dropna()
one_hot = pd.get_dummies(copd_rq2['Stratification1'])
copd_rq2 = copd_rq2.join(one_hot)
copd_rq2 = copd_rq2.rename(columns={'YearStart':'Year','Black, non-Hispanic':"Black", "Multiracial, non-Hispanic" : "Multiracial", "Other, non-Hispanic": "Other", "White, non-Hispanic":"White"})
copd_rq2['DataValue'] = copd_rq2['DataValue'].astype(float)
copd_rq2['likely'] = 1*(copd_rq2['DataValue'] > 6.4)
copd_rq2 = copd_rq2.reset_index().drop(columns=['Stratification1', 'DataValue','index'])
copd_rq2 = copd_rq2.merge(longlat, left_on = 'LocationAbbr', right_on = 'state').drop(columns=['state'])
copd_rq2_NN = copd_rq2.drop(columns=['LocationAbbr', 'name'])
copd_rq2_NN.head()

"""###Train/Test split & Fitting Random Forest Model"""

# Train/Test split & Fitting Random Forest Model
from sklearn.model_selection import train_test_split

train, test = train_test_split(copd_rq2_NN, test_size=.3, random_state=101)
X_cols = ['Year', 'Black', 'Hispanic', 'Multiracial', 'Other', 'White','latitude', 'longitude']
y_col = 'likely'
from sklearn.ensemble import RandomForestRegressor

forest_model = RandomForestRegressor(max_features=2)
forest_model.fit(train[X_cols], train[y_col])
train['forest_pred'] = forest_model.predict(train[X_cols])
test['forest_pred'] = forest_model.predict(test[X_cols])

"""###Error Calculation: RMSE"""

train_rmse = np.mean((train["forest_pred"] - train["likely"]) ** 2) ** 0.5
test_rmse = np.mean((test["forest_pred"] - test["likely"]) ** 2) ** 0.5

print("Training set error for random forest:", train_rmse)
print("Test set error for random forest:    ", test_rmse)

"""###Evaluation: Compute TPR & TNR"""

test_trueCOPD = test[test['likely'] == 1]
test_predTrue = test_trueCOPD[test_trueCOPD['forest_pred'] > 0.60]
TPR = len(test_predTrue)/ len(test_trueCOPD)

test_falseCOPD = test[test['likely'] == 0]
test_predFalse = test_falseCOPD[test_falseCOPD['forest_pred'] <= 0.40]
TNR = len(test_predFalse)/ len(test_falseCOPD)

print("Test set True Positive Rate:", TPR)
print("Test set True Negative Rate:    ", TNR)

train_trueCOPD = train[train['likely'] == 1]
train_predTrue = train_trueCOPD[train_trueCOPD['forest_pred'] > 0.60]
TPR = len(train_predTrue)/ len(train_trueCOPD)

train_falseCOPD = train[train['likely'] == 0]
train_predFalse = train_falseCOPD[train_falseCOPD['forest_pred'] <= 0.40]
TNR = len(train_predFalse)/ len(train_falseCOPD)

print("Train set True Positive Rate:", TPR)
print("Train set True Negative Rate:    ", TNR)

"""###Visualization of Random Forest Model"""

from sklearn.tree import export_graphviz
import pydot
# Pull out one tree from the forest
tree = forest_model.estimators_[5]
# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = X_cols, rounded = True, precision = 1)
# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')

from IPython.display import Image 
Image(graph.create_png())

"""###Visualization of a Single Decision Tree"""

# Take a closer look into a single decision tree; Limit depth of tree to 3 levels
RF = RandomForestRegressor(n_estimators=20, max_depth = 3)
RF.fit(train[X_cols], train[y_col])
# Extract the small tree
smallTree = RF.estimators_[5]
# Save the tree as a png image
export_graphviz(smallTree, out_file = 'smallTree.dot', feature_names = X_cols, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('smallTree.dot')
Image(graph.create_png())

"""###Computing Feature Importance"""

# Get numerical feature importances
importances = list(forest_model.feature_importances_)
# Sort the feature importances
feature_imp = sorted([(feature, round(importance, 2)) for feature, importance in zip(X_cols, importances)], key = lambda x: x[1], reverse = True)
# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*tup)) for tup in feature_imp];

"""###Results

**Train/Test Error (RMSE) & Accuracy (TPR/TNR)**

While both training and test set error are relatively low, test set error is slightly greater than training set error. To better understand the prediction, we computed true positive rate and true negative rate for both test and train set. Here, we defined predictions greater than 60% as 'predicting that one has COPD'. Similarly, we defined predictions less than 40% as 'predicting that one does not have COPD'. TPR and TNR of our train set are approximately 99%, while that of our test set are approximately 81%. Overall, our model accuracy is high and our model error is also relatively low. 

**Visualization of a Single Decision Tree**

As shown in the visualization of a single decision tree, the root node first looks at multiracial feature to split the data. Then, its children split after looking into longitude and latitude of the data. For instance, the rightmost branch predicts higher value when multiracial feature is greater than 0.5 and latitude is greater than 23.8. Similarly, when the latitude is less than or equal to 23.8, model predicts lower value. The only states that meet this condition (Guam, Hawaii, and Puerto Rico) are the states with lowest age-adjusted prevalence rate of COPD. This indicates that the model is incorporating geographical locations to accurately predict COPD. 

Although this is a simple random forest model generated to only show a single decision tree, it seems like most of the layers make decisions based on longitude, latitude, multiracial, and year. 

**Computing Feature Importance**

To further examine features and their role in prediction, we computed feature importances. As shown below, longitude is the best predictor of whether or not an individual will be diagnosed with COPD. Then follows latitude, Year, Multiracial, and more. This indicates that geographic location is the best predictor overall.
"""
